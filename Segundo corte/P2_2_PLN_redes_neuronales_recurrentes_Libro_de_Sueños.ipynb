{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2_2_PLN_redes_neuronales_recurrentes_Libro_de_Sueños.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeX0VDYrGnvT"
      },
      "source": [
        "#P0. Introducción -PLN\n",
        "\n",
        "---\n",
        "\n",
        "En la creación de redes neuronales necesitamos dos tipos de IA, para reconocer patrones o generar nuevos:\n",
        "\n",
        "*   Las que no tienen memoria, identifica el patrón y ya!...ejemplo las de visión artificial\n",
        "*   Las de memoria corta (Long Short Term Memory)...PLN\n",
        "*   Las que requieren mucha memoria (aprenden casi todo...BERT)...PLN y visión artificial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7wqUcntGrap"
      },
      "source": [
        "**Caso de estudio: generación de texto**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Cualquier dato que se necesite procesar (sonido, imágenes, texto) primero debe ser convertido en un tensor numérico, un paso llamado “vectorización” (One-hot Encoding y WordEmbedding) de datos (y en nuestro ejemplo previamente las letras deben ser pasadas a valores numéricos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MDO7qGFGwXr"
      },
      "source": [
        "Para este ejemplo usaremos “*Character level language model*” propuesto por Andrej Karpathy en su artículo \"*The Unreasonable Effectiveness of Recurrent Neural Networks*\"(y parcialmente basado en su implementado en el tutorial \"*Generate text with an RNN*\" de la web de TensorFlow:\n",
        "\n",
        "Consiste en darle a la RNN una palabra y se le pide que modele la distribución de probabilidad del siguiente carácter que le correspondería a la secuencia de caracteres anteriores:\n",
        "\n",
        "Como ejemplo, supongamos que solo tenemos un vocabulario de cuatro letras posibles [“a”,”h”,”l”,”o”], y queremos entrenar a una RNN en la secuencia de entrenamiento “hola”. Esta secuencia de entrenamiento es, de hecho, una fuente de 3 ejemplos de entrenamiento por separado: La probabilidad de “o” debería ser verosímil dada el contexto de “h”, “l” debería ser verosímil en el contexto de “ho”, y finalmente “a” debería ser también verosímil dado el contexto de “hol”.\n",
        "\n",
        "\n",
        "---\n",
        "https://unipython.com/generacion-de-textos-con-inteligencia-artificial/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4riLS78nG02k"
      },
      "source": [
        "#Ejemplo 2: generar texto de cuentos, usando Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oVZM7wXG5Y5"
      },
      "source": [
        "##P0. importar librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrFvaUZFGj0v"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgjdxXGgHYB3"
      },
      "source": [
        "##P0. Descarga y preprocesado de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJdLZ3W2HXQo",
        "outputId": "fd7d1273-8f22-400f-ca7c-26c15bf865e0"
      },
      "source": [
        "fileDL= tf.keras.utils.get_file('LaInterpretacionDeLosSueñosSigmundFreud.txt','https://raw.githubusercontent.com/Moisesdv10/Deep-Learning/main/Segundo%20corte/PLN/Datasets/Panel_Txt_Files/LaInterpretacionDeLosSue%C3%B1osSigmundFreud.txt')\n",
        "texto = open(fileDL, 'rb').read().decode(encoding='utf-8')\n",
        "texto = texto.lower()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/Moisesdv10/Deep-Learning/main/Segundo%20corte/PLN/Datasets/Panel_Txt_Files/LaInterpretacionDeLosSue%C3%B1osSigmundFreud.txt\n",
            "1302528/1300981 [==============================] - 0s 0us/step\n",
            "1310720/1300981 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80IzUHQXHfY4"
      },
      "source": [
        "##P1. entendiendo el texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8_RRSEBHf9r",
        "outputId": "3c90f4da-30bb-4401-de79-d36a9ad00a51"
      },
      "source": [
        "print('el texto tiene longitud de:{} caracteres'. format(len(texto)))\n",
        "vocab = sorted(set(texto))\n",
        "print('el texto esta compuesto de estos :{} caracteres'. format(len(vocab)))\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el texto tiene longitud de:1293168 caracteres\n",
            "el texto esta compuesto de estos :80 caracteres\n",
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', '+', ',', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', 'ª', '«', 'º', '»', '¿', 'ß', 'à', 'â', 'ä', 'æ', 'ç', 'è', 'ê', 'î', 'ñ', 'ô', 'ö', 'û', 'ü', '…', '�']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcRMJwSuHkLh"
      },
      "source": [
        "##P2. pasar el texto a números\n",
        "\n",
        "---\n",
        "as redes neuronales solo procesan valores numéricos, no letras, por tanto tenemos que traducir los caracteres a representación numérica. Para ello crearemos dos “tablas de traducción”: una de caracteres a números y otra de números a caracteres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev2NREYrHl_3",
        "outputId": "74cc76d8-3d08-4afd-c7f1-040628308586"
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)} # asignamos un número a cada vocablo\n",
        "idx2char = np.array(vocab)\n",
        "#-----------revisando las conversiones\n",
        "#for char,_ in zip(char2idx, range(len(vocab))):\n",
        "#    print(' {:4s}: {:3d},'.format(repr(char),char2idx[char]))\n",
        "\n",
        "#pasamos todo el texto a números\n",
        "texto_como_entero= np.array([char2idx[c] for c in texto])\n",
        "print('texto: {}'.format(repr(texto[:100])))\n",
        "print('{}'.format(repr(texto_como_entero[:100])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texto: 'la interpretacion de los sueños         \\n    sigmund freud \\nsigmund freud \\nla interpretacion de los '\n",
            "array([43, 32,  1, 40, 45, 51, 36, 49, 47, 49, 36, 51, 32, 34, 40, 46, 45,\n",
            "        1, 35, 36,  1, 43, 46, 50,  1, 50, 52, 36, 73, 46, 50,  1,  1,  1,\n",
            "        1,  1,  1,  1,  1,  1,  0,  1,  1,  1,  1, 50, 40, 38, 44, 52, 45,\n",
            "       35,  1, 37, 49, 36, 52, 35,  1,  0, 50, 40, 38, 44, 52, 45, 35,  1,\n",
            "       37, 49, 36, 52, 35,  1,  0, 43, 32,  1, 40, 45, 51, 36, 49, 47, 49,\n",
            "       36, 51, 32, 34, 40, 46, 45,  1, 35, 36,  1, 43, 46, 50,  1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV6V0fiAHpI7"
      },
      "source": [
        "##P3. preparar los datos para ser usados en la RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TADpx2lIHp7R",
        "outputId": "0bdaa376-5f1c-4817-d20c-56bc19877d68"
      },
      "source": [
        "char_dataset= tf.data.Dataset.from_tensor_slices(texto_como_entero)\n",
        "#cantidad de secuencia de caracteres\n",
        "secu_length=150\n",
        "#creamos secuencias de maximo 100 caractereres\n",
        "secuencias= char_dataset.batch(secu_length+1, drop_remainder=True)\n",
        "for item in secuencias.take(10):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'la interpretacion de los sueños         \\n    sigmund freud \\nsigmund freud \\nla interpretacion de los sueños \\n  \\n18989 [1900] \\nla interpretacion de los s'\n",
            "'ueños         \\n    sigmund freud \\nprefacio a la primera edicion \\n(1900) \\nal proponerme exponer la interpretacion de los sueños no creo haber trascendid'\n",
            "'o los \\nambitos del interes neuropatologico, pues, el examen psicologico nos presenta el sueño co\\nmo primer eslabon de una serie de fenomenos psiquicos '\n",
            "'anormales, entre cuyos elementos \\nsubsiguientes, las fobias histericas y las formaciones obsesivas y delirantes, conciernen al \\nmedico por motivos prac'\n",
            "'ticos. desde luego, como ya lo demostraremos, el sueño no puede \\npretender analoga importancia practica; pero tanto mayor es su valor teorico como para'\n",
            "'dig\\nma, al punto que quien no logre explicarse la genesis de las imagenes oniricas, se esforzara \\nen vano por comprender las fobias, las ideas obsesiva'\n",
            "'s, los delirios, y por ejercer sobre esa \\nestos fenomenos un posible influjo terapeutico. \\n mas precisamente esta vinculacion, a la que nuestro tema de'\n",
            "'be toda su importancia, es \\ntambien el motivo de los defectos de que adolece el presente trabajo, pues el frecuente ca\\nracter fragmentario de su exposi'\n",
            "'cion corresponde a otros tantos puntos de contacto, a cuyo \\nnivel los problemas de la formacion onirica toman injerencia en los problemas mas amplios \\n'\n",
            "'de la psicopatologia, que no pudieron se considerados en esta ocasion y que seran motivo \\nde trabajos futuros, siempre que para ello alcancen el tiempo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0FiQ5KfHtRK"
      },
      "source": [
        "###P3.1 separar los datos en agrupamientos (batches)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lps8zcdHHty_",
        "outputId": "90208e87-8dd2-4bc2-cc66-f0831e16038f"
      },
      "source": [
        "#funcion para obtener el conjunto de datos de trainning\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text= chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset  = secuencias.map(split_input_target)\n",
        "#el dataset contiene un conjunto de parejas de secuencia de texto\n",
        "#(con la representación numérica de los caracteres), donde el \n",
        "#primer componente de la pareja contiene un paquete con una secuencia \n",
        "#de 100 caracteres del texto original y la segunda su correspondiente salida, \n",
        "#también de 100 caracteres. )\n",
        "for input_example, target_example in dataset.take(1):\n",
        "  print('input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Target data: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input data:  'la interpretacion de los sueños         \\n    sigmund freud \\nsigmund freud \\nla interpretacion de los sueños \\n  \\n18989 [1900] \\nla interpretacion de los '\n",
            "Target data:  'a interpretacion de los sueños         \\n    sigmund freud \\nsigmund freud \\nla interpretacion de los sueños \\n  \\n18989 [1900] \\nla interpretacion de los s'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPcYfLAqHw5w",
        "outputId": "ef468d6b-86c2-4975-dc30-d1d42da00919"
      },
      "source": [
        "#imprimimos el tensor del dataset\n",
        "print(dataset)\n",
        "#Hyper-Parametros para entrenamiento  de una rede neuronal \n",
        "#   -los datos se agrupan en batch\n",
        "BATCH_SIZE= 64\n",
        "#    -Tamaño de memoria disponible \n",
        "BUFFER_SIZE=10000\n",
        "dataset= dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print (dataset)\n",
        "#En el tensor dataset disponemos los datos de entrenamiento\n",
        "#con agrupamienttos (batches) compuestos de 64 parejas de secuencias \n",
        "#de 100 integers de 64 bits que representan el carácter correspondiente \n",
        "#en el vocabulario."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MapDataset shapes: ((150,), (150,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((64, 150), (64, 150)), types: (tf.int64, tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcZbIlUtHyzt"
      },
      "source": [
        "##P4.Construcción del modelo RNN\n",
        "\n",
        "---\n",
        "Para construir el modelo usaremos tf.keras.Sequential. Usaremos una versión mínima de RNN, que contenga solo una capa LSTM y 3 capas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkRs9KmmHzYj",
        "outputId": "67949b8c-ccae-45e1-fa1c-39d301836aef"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  #creando el modelo\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)                               \n",
        "  ])\n",
        "  return model\n",
        "vocab_size= len(vocab)\n",
        "#dimensiones de los vectores que tendrá la capa.\n",
        "embedding_dim= 256\n",
        "#cantidad de neuronas\n",
        "rnn_units=1024\n",
        "#creamos nuestra red neuronal RNN\n",
        "model=build_model(vocab_size=vocab_size,\n",
        "                  embedding_dim=embedding_dim,\n",
        "                  rnn_units=rnn_units,\n",
        "                  batch_size=BATCH_SIZE)\n",
        "#summary()para visualizar la estructura del modelo\n",
        "model.summary()\n",
        "#resultados=\n",
        "#    -La capa LSTM consta más de 5 millones de parametros)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           20480     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 80)            82000     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,349,456\n",
            "Trainable params: 5,349,456\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NStgtsH3kn"
      },
      "source": [
        "##P4. Entrenando la RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka2nmX8-H5L3"
      },
      "source": [
        "#como es un problema de clasificación estándar \n",
        "#para el que debemos definir la función de Lossy el optimizador.\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "#En cuanto al optimizador usaremos tf.keras.optimizers.Adam \n",
        "#con los argumentos por defecto del optimizador Adam.  \n",
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwuETxEHH8R0"
      },
      "source": [
        "###P4.1 Creando chekpoints\n",
        "\n",
        "---\n",
        "una técnica de tolerancia de fallos para procesos cuyo tiempo de ejecución es muy largo. La idea es guardar una instantánea del estado del sistema periódicamente para recuperar desde ese punto la ejecución en caso de fallo del sistema.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JV3eY3xH82g"
      },
      "source": [
        "checkpoint_dir='/content/checkpointV2'\n",
        "checkpoint_prefix= os.path.join(checkpoint_dir,\"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55orpl2pICM6"
      },
      "source": [
        "###P4.2 entrenando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXiKQuWEH_-8",
        "outputId": "178da0a5-f376-4cb4-9e32-d4c7b0a16b59"
      },
      "source": [
        "EPOCHS=500\n",
        "history=model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "#model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "133/133 [==============================] - 39s 231ms/step - loss: 2.4922\n",
            "Epoch 2/500\n",
            "133/133 [==============================] - 33s 234ms/step - loss: 1.9017\n",
            "Epoch 3/500\n",
            "133/133 [==============================] - 33s 236ms/step - loss: 1.5886\n",
            "Epoch 4/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 1.4000\n",
            "Epoch 5/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 1.2928\n",
            "Epoch 6/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 1.2252\n",
            "Epoch 7/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 1.1777\n",
            "Epoch 8/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 1.1404\n",
            "Epoch 9/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 1.1061\n",
            "Epoch 10/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 1.0749\n",
            "Epoch 11/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 1.0444\n",
            "Epoch 12/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 1.0169\n",
            "Epoch 13/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.9923\n",
            "Epoch 14/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.9668\n",
            "Epoch 15/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.9410\n",
            "Epoch 16/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.9139\n",
            "Epoch 17/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.8875\n",
            "Epoch 18/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.8600\n",
            "Epoch 19/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8306\n",
            "Epoch 20/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.8032\n",
            "Epoch 21/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.7734\n",
            "Epoch 22/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.7445\n",
            "Epoch 23/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.7151\n",
            "Epoch 24/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.6866\n",
            "Epoch 25/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.6588\n",
            "Epoch 26/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.6336\n",
            "Epoch 27/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.6071\n",
            "Epoch 28/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.5837\n",
            "Epoch 29/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.5593\n",
            "Epoch 30/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.5405\n",
            "Epoch 31/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.5199\n",
            "Epoch 32/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.5017\n",
            "Epoch 33/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4845\n",
            "Epoch 34/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4689\n",
            "Epoch 35/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4560\n",
            "Epoch 36/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4418\n",
            "Epoch 37/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.4285\n",
            "Epoch 38/500\n",
            "133/133 [==============================] - 34s 242ms/step - loss: 0.4173\n",
            "Epoch 39/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.4072\n",
            "Epoch 40/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3991\n",
            "Epoch 41/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3903\n",
            "Epoch 42/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3823\n",
            "Epoch 43/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3730\n",
            "Epoch 44/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3671\n",
            "Epoch 45/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3608\n",
            "Epoch 46/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3556\n",
            "Epoch 47/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3499\n",
            "Epoch 48/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3472\n",
            "Epoch 49/500\n",
            "133/133 [==============================] - 34s 238ms/step - loss: 0.3414\n",
            "Epoch 50/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3366\n",
            "Epoch 51/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3321\n",
            "Epoch 52/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3292\n",
            "Epoch 53/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3264\n",
            "Epoch 54/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3223\n",
            "Epoch 55/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3184\n",
            "Epoch 56/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3173\n",
            "Epoch 57/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3144\n",
            "Epoch 58/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3114\n",
            "Epoch 59/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3082\n",
            "Epoch 60/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3098\n",
            "Epoch 61/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3079\n",
            "Epoch 62/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3026\n",
            "Epoch 63/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3023\n",
            "Epoch 64/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3008\n",
            "Epoch 65/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2999\n",
            "Epoch 66/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2969\n",
            "Epoch 67/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2943\n",
            "Epoch 68/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2939\n",
            "Epoch 69/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2935\n",
            "Epoch 70/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2917\n",
            "Epoch 71/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2917\n",
            "Epoch 72/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2897\n",
            "Epoch 73/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2877\n",
            "Epoch 74/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2882\n",
            "Epoch 75/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2842\n",
            "Epoch 76/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2849\n",
            "Epoch 77/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2831\n",
            "Epoch 78/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2832\n",
            "Epoch 79/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2810\n",
            "Epoch 80/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2816\n",
            "Epoch 81/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2807\n",
            "Epoch 82/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2796\n",
            "Epoch 83/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2787\n",
            "Epoch 84/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2780\n",
            "Epoch 85/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2772\n",
            "Epoch 86/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2760\n",
            "Epoch 87/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2764\n",
            "Epoch 88/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2760\n",
            "Epoch 89/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2764\n",
            "Epoch 90/500\n",
            "133/133 [==============================] - 34s 242ms/step - loss: 0.2738\n",
            "Epoch 91/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2726\n",
            "Epoch 92/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2722\n",
            "Epoch 93/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2705\n",
            "Epoch 94/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2706\n",
            "Epoch 95/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2696\n",
            "Epoch 96/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2688\n",
            "Epoch 97/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2681\n",
            "Epoch 98/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2690\n",
            "Epoch 99/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2663\n",
            "Epoch 100/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2667\n",
            "Epoch 101/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2663\n",
            "Epoch 102/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2651\n",
            "Epoch 103/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2666\n",
            "Epoch 104/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2666\n",
            "Epoch 105/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2671\n",
            "Epoch 106/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2662\n",
            "Epoch 107/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2638\n",
            "Epoch 108/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2627\n",
            "Epoch 109/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2615\n",
            "Epoch 110/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2615\n",
            "Epoch 111/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2622\n",
            "Epoch 112/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2603\n",
            "Epoch 113/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2589\n",
            "Epoch 114/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2578\n",
            "Epoch 115/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2574\n",
            "Epoch 116/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2605\n",
            "Epoch 117/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2584\n",
            "Epoch 118/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2598\n",
            "Epoch 119/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2574\n",
            "Epoch 120/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2586\n",
            "Epoch 121/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2596\n",
            "Epoch 122/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2603\n",
            "Epoch 123/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2613\n",
            "Epoch 124/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2580\n",
            "Epoch 125/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2568\n",
            "Epoch 126/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2558\n",
            "Epoch 127/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2543\n",
            "Epoch 128/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2554\n",
            "Epoch 129/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2567\n",
            "Epoch 130/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2566\n",
            "Epoch 131/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2542\n",
            "Epoch 132/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2533\n",
            "Epoch 133/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2527\n",
            "Epoch 134/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2513\n",
            "Epoch 135/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2516\n",
            "Epoch 136/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2531\n",
            "Epoch 137/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2544\n",
            "Epoch 138/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2531\n",
            "Epoch 139/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2516\n",
            "Epoch 140/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2504\n",
            "Epoch 141/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2506\n",
            "Epoch 142/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2506\n",
            "Epoch 143/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2507\n",
            "Epoch 144/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2492\n",
            "Epoch 145/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2501\n",
            "Epoch 146/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2491\n",
            "Epoch 147/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2493\n",
            "Epoch 148/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2491\n",
            "Epoch 149/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2506\n",
            "Epoch 150/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2498\n",
            "Epoch 151/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2493\n",
            "Epoch 152/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2492\n",
            "Epoch 153/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2490\n",
            "Epoch 154/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2483\n",
            "Epoch 155/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2467\n",
            "Epoch 156/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2456\n",
            "Epoch 157/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2486\n",
            "Epoch 158/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2468\n",
            "Epoch 159/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2498\n",
            "Epoch 160/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2490\n",
            "Epoch 161/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2485\n",
            "Epoch 162/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2468\n",
            "Epoch 163/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2466\n",
            "Epoch 164/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2467\n",
            "Epoch 165/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2461\n",
            "Epoch 166/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2449\n",
            "Epoch 167/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2446\n",
            "Epoch 168/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2432\n",
            "Epoch 169/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2440\n",
            "Epoch 170/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2424\n",
            "Epoch 171/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2421\n",
            "Epoch 172/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2432\n",
            "Epoch 173/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2457\n",
            "Epoch 174/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2452\n",
            "Epoch 175/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2479\n",
            "Epoch 176/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2470\n",
            "Epoch 177/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2456\n",
            "Epoch 178/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2452\n",
            "Epoch 179/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2450\n",
            "Epoch 180/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2435\n",
            "Epoch 181/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2433\n",
            "Epoch 182/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2442\n",
            "Epoch 183/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2431\n",
            "Epoch 184/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2397\n",
            "Epoch 185/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2410\n",
            "Epoch 186/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2412\n",
            "Epoch 187/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2393\n",
            "Epoch 188/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2387\n",
            "Epoch 189/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2385\n",
            "Epoch 190/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2389\n",
            "Epoch 191/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2416\n",
            "Epoch 192/500\n",
            "133/133 [==============================] - 34s 242ms/step - loss: 0.2418\n",
            "Epoch 193/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2485\n",
            "Epoch 194/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2653\n",
            "Epoch 195/500\n",
            "133/133 [==============================] - 34s 242ms/step - loss: 0.3001\n",
            "Epoch 196/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2937\n",
            "Epoch 197/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2757\n",
            "Epoch 198/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2568\n",
            "Epoch 199/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2419\n",
            "Epoch 200/500\n",
            "133/133 [==============================] - 34s 242ms/step - loss: 0.2291\n",
            "Epoch 201/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2215\n",
            "Epoch 202/500\n",
            "133/133 [==============================] - 34s 242ms/step - loss: 0.2149\n",
            "Epoch 203/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2097\n",
            "Epoch 204/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2088\n",
            "Epoch 205/500\n",
            "133/133 [==============================] - 34s 242ms/step - loss: 0.2106\n",
            "Epoch 206/500\n",
            "133/133 [==============================] - 34s 242ms/step - loss: 0.2145\n",
            "Epoch 207/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2204\n",
            "Epoch 208/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2311\n",
            "Epoch 209/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2459\n",
            "Epoch 210/500\n",
            "133/133 [==============================] - 34s 242ms/step - loss: 0.2691\n",
            "Epoch 211/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2951\n",
            "Epoch 212/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3149\n",
            "Epoch 213/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3094\n",
            "Epoch 214/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2870\n",
            "Epoch 215/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2653\n",
            "Epoch 216/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2464\n",
            "Epoch 217/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2327\n",
            "Epoch 218/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2218\n",
            "Epoch 219/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2135\n",
            "Epoch 220/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2096\n",
            "Epoch 221/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2069\n",
            "Epoch 222/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2083\n",
            "Epoch 223/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2092\n",
            "Epoch 224/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2130\n",
            "Epoch 225/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2194\n",
            "Epoch 226/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2329\n",
            "Epoch 227/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2529\n",
            "Epoch 228/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2867\n",
            "Epoch 229/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3174\n",
            "Epoch 230/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3306\n",
            "Epoch 231/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3177\n",
            "Epoch 232/500\n",
            "133/133 [==============================] - 34s 242ms/step - loss: 0.2922\n",
            "Epoch 233/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2718\n",
            "Epoch 234/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2521\n",
            "Epoch 235/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2360\n",
            "Epoch 236/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2233\n",
            "Epoch 237/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2158\n",
            "Epoch 238/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2084\n",
            "Epoch 239/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2058\n",
            "Epoch 240/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2022\n",
            "Epoch 241/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2030\n",
            "Epoch 242/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2053\n",
            "Epoch 243/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2109\n",
            "Epoch 244/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2225\n",
            "Epoch 245/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2400\n",
            "Epoch 246/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2719\n",
            "Epoch 247/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3119\n",
            "Epoch 248/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3407\n",
            "Epoch 249/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3446\n",
            "Epoch 250/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3251\n",
            "Epoch 251/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2941\n",
            "Epoch 252/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2690\n",
            "Epoch 253/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2493\n",
            "Epoch 254/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2328\n",
            "Epoch 255/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2201\n",
            "Epoch 256/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2120\n",
            "Epoch 257/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2056\n",
            "Epoch 258/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2019\n",
            "Epoch 259/500\n",
            "133/133 [==============================] - 34s 238ms/step - loss: 0.1996\n",
            "Epoch 260/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2008\n",
            "Epoch 261/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2016\n",
            "Epoch 262/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2069\n",
            "Epoch 263/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2167\n",
            "Epoch 264/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2390\n",
            "Epoch 265/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2853\n",
            "Epoch 266/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3559\n",
            "Epoch 267/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4020\n",
            "Epoch 268/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3807\n",
            "Epoch 269/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3449\n",
            "Epoch 270/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3062\n",
            "Epoch 271/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2793\n",
            "Epoch 272/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2563\n",
            "Epoch 273/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2380\n",
            "Epoch 274/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2258\n",
            "Epoch 275/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2141\n",
            "Epoch 276/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2077\n",
            "Epoch 277/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2026\n",
            "Epoch 278/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2002\n",
            "Epoch 279/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.1992\n",
            "Epoch 280/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2001\n",
            "Epoch 281/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2013\n",
            "Epoch 282/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2099\n",
            "Epoch 283/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2214\n",
            "Epoch 284/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2438\n",
            "Epoch 285/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2900\n",
            "Epoch 286/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3583\n",
            "Epoch 287/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3952\n",
            "Epoch 288/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3928\n",
            "Epoch 289/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3589\n",
            "Epoch 290/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3223\n",
            "Epoch 291/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2959\n",
            "Epoch 292/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2735\n",
            "Epoch 293/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2539\n",
            "Epoch 294/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2374\n",
            "Epoch 295/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2260\n",
            "Epoch 296/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2167\n",
            "Epoch 297/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2102\n",
            "Epoch 298/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2058\n",
            "Epoch 299/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2022\n",
            "Epoch 300/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2026\n",
            "Epoch 301/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2017\n",
            "Epoch 302/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2058\n",
            "Epoch 303/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2113\n",
            "Epoch 304/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2240\n",
            "Epoch 305/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2510\n",
            "Epoch 306/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3056\n",
            "Epoch 307/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3823\n",
            "Epoch 308/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.4176\n",
            "Epoch 309/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3956\n",
            "Epoch 310/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3616\n",
            "Epoch 311/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3360\n",
            "Epoch 312/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3090\n",
            "Epoch 313/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2815\n",
            "Epoch 314/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2727\n",
            "Epoch 315/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2550\n",
            "Epoch 316/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2408\n",
            "Epoch 317/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2300\n",
            "Epoch 318/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2226\n",
            "Epoch 319/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2167\n",
            "Epoch 320/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2122\n",
            "Epoch 321/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2088\n",
            "Epoch 322/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2083\n",
            "Epoch 323/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2085\n",
            "Epoch 324/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2111\n",
            "Epoch 325/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2166\n",
            "Epoch 326/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2309\n",
            "Epoch 327/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2566\n",
            "Epoch 328/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2972\n",
            "Epoch 329/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3482\n",
            "Epoch 330/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3939\n",
            "Epoch 331/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3959\n",
            "Epoch 332/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3698\n",
            "Epoch 333/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3494\n",
            "Epoch 334/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3204\n",
            "Epoch 335/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2974\n",
            "Epoch 336/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2757\n",
            "Epoch 337/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2588\n",
            "Epoch 338/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2459\n",
            "Epoch 339/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2360\n",
            "Epoch 340/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2298\n",
            "Epoch 341/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2241\n",
            "Epoch 342/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2209\n",
            "Epoch 343/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2165\n",
            "Epoch 344/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2177\n",
            "Epoch 345/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2168\n",
            "Epoch 346/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2213\n",
            "Epoch 347/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2280\n",
            "Epoch 348/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2403\n",
            "Epoch 349/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2612\n",
            "Epoch 350/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.2907\n",
            "Epoch 351/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3340\n",
            "Epoch 352/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3671\n",
            "Epoch 353/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3828\n",
            "Epoch 354/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3818\n",
            "Epoch 355/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3659\n",
            "Epoch 356/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3479\n",
            "Epoch 357/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3235\n",
            "Epoch 358/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3004\n",
            "Epoch 359/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2803\n",
            "Epoch 360/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2673\n",
            "Epoch 361/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2529\n",
            "Epoch 362/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2418\n",
            "Epoch 363/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2333\n",
            "Epoch 364/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2266\n",
            "Epoch 365/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2219\n",
            "Epoch 366/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2172\n",
            "Epoch 367/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2169\n",
            "Epoch 368/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2168\n",
            "Epoch 369/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2194\n",
            "Epoch 370/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2282\n",
            "Epoch 371/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2408\n",
            "Epoch 372/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2674\n",
            "Epoch 373/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3087\n",
            "Epoch 374/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3681\n",
            "Epoch 375/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4079\n",
            "Epoch 376/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4173\n",
            "Epoch 377/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.4081\n",
            "Epoch 378/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3887\n",
            "Epoch 379/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3651\n",
            "Epoch 380/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.3555\n",
            "Epoch 381/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3280\n",
            "Epoch 382/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3080\n",
            "Epoch 383/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2912\n",
            "Epoch 384/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2780\n",
            "Epoch 385/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2673\n",
            "Epoch 386/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2579\n",
            "Epoch 387/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2515\n",
            "Epoch 388/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2467\n",
            "Epoch 389/500\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.2409\n",
            "Epoch 390/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2364\n",
            "Epoch 391/500\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.2362\n",
            "Epoch 392/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2356\n",
            "Epoch 393/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2368\n",
            "Epoch 394/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2422\n",
            "Epoch 395/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2529\n",
            "Epoch 396/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2664\n",
            "Epoch 397/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2949\n",
            "Epoch 398/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3219\n",
            "Epoch 399/500\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.3659\n",
            "Epoch 400/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3986\n",
            "Epoch 401/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.4288\n",
            "Epoch 402/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4149\n",
            "Epoch 403/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.4012\n",
            "Epoch 404/500\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.3836\n",
            "Epoch 405/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3665\n",
            "Epoch 406/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3527\n",
            "Epoch 407/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.3382\n",
            "Epoch 408/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3256\n",
            "Epoch 409/500\n",
            "133/133 [==============================] - 34s 238ms/step - loss: 0.3154\n",
            "Epoch 410/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3092\n",
            "Epoch 411/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.3002\n",
            "Epoch 412/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2973\n",
            "Epoch 413/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.2879\n",
            "Epoch 414/500\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.2846\n",
            "Epoch 415/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2775\n",
            "Epoch 416/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2781\n",
            "Epoch 417/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.2777\n",
            "Epoch 418/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.2787\n",
            "Epoch 419/500\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.2798\n",
            "Epoch 420/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.2919\n",
            "Epoch 421/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.3013\n",
            "Epoch 422/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3118\n",
            "Epoch 423/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.3259\n",
            "Epoch 424/500\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.3410\n",
            "Epoch 425/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3621\n",
            "Epoch 426/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3845\n",
            "Epoch 427/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3876\n",
            "Epoch 428/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.3931\n",
            "Epoch 429/500\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.4047\n",
            "Epoch 430/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.4019\n",
            "Epoch 431/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.4085\n",
            "Epoch 432/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4077\n",
            "Epoch 433/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3991\n",
            "Epoch 434/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3884\n",
            "Epoch 435/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.3872\n",
            "Epoch 436/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3816\n",
            "Epoch 437/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3743\n",
            "Epoch 438/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.3707\n",
            "Epoch 439/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.3684\n",
            "Epoch 440/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3729\n",
            "Epoch 441/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.3756\n",
            "Epoch 442/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.4064\n",
            "Epoch 443/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4224\n",
            "Epoch 444/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.4180\n",
            "Epoch 445/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4383\n",
            "Epoch 446/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.4332\n",
            "Epoch 447/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4456\n",
            "Epoch 448/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4474\n",
            "Epoch 449/500\n",
            "133/133 [==============================] - 34s 238ms/step - loss: 0.4430\n",
            "Epoch 450/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4413\n",
            "Epoch 451/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4474\n",
            "Epoch 452/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4639\n",
            "Epoch 453/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4635\n",
            "Epoch 454/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.4755\n",
            "Epoch 455/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.5017\n",
            "Epoch 456/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.5253\n",
            "Epoch 457/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.5179\n",
            "Epoch 458/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.5519\n",
            "Epoch 459/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.5424\n",
            "Epoch 460/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.5342\n",
            "Epoch 461/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.5390\n",
            "Epoch 462/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.5601\n",
            "Epoch 463/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.5879\n",
            "Epoch 464/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.5990\n",
            "Epoch 465/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.5998\n",
            "Epoch 466/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.6139\n",
            "Epoch 467/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.6072\n",
            "Epoch 468/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.6026\n",
            "Epoch 469/500\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.6013\n",
            "Epoch 470/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.6278\n",
            "Epoch 471/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.6384\n",
            "Epoch 472/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.6479\n",
            "Epoch 473/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.6981\n",
            "Epoch 474/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.6673\n",
            "Epoch 475/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.6873\n",
            "Epoch 476/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.6996\n",
            "Epoch 477/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.7037\n",
            "Epoch 478/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.7172\n",
            "Epoch 479/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.7227\n",
            "Epoch 480/500\n",
            "133/133 [==============================] - 33s 240ms/step - loss: 0.7258\n",
            "Epoch 481/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.7522\n",
            "Epoch 482/500\n",
            "133/133 [==============================] - 34s 241ms/step - loss: 0.7765\n",
            "Epoch 483/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.7833\n",
            "Epoch 484/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.7769\n",
            "Epoch 485/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.7829\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.7829\n",
            "Epoch 486/500\n",
            "Epoch 486/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.7829\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.7829\n",
            "Epoch 487/500\n",
            "Epoch 487/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8104\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8104\n",
            "Epoch 488/500\n",
            "Epoch 488/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8276\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8276\n",
            "Epoch 489/500\n",
            "Epoch 489/500\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.8370\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.8370\n",
            "Epoch 490/500\n",
            "Epoch 490/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8678\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8678\n",
            "Epoch 491/500\n",
            "Epoch 491/500\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.8318\n",
            "133/133 [==============================] - 33s 239ms/step - loss: 0.8318\n",
            "Epoch 492/500\n",
            "Epoch 492/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8163\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8163\n",
            "Epoch 493/500\n",
            "Epoch 493/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.8164\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.8164\n",
            "Epoch 494/500\n",
            "Epoch 494/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8221\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8221\n",
            "Epoch 495/500\n",
            "Epoch 495/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8395\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8395\n",
            "Epoch 496/500\n",
            "Epoch 496/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8758\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.8758\n",
            "Epoch 497/500\n",
            "Epoch 497/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.9264\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.9264\n",
            "Epoch 498/500\n",
            "Epoch 498/500\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.9122\n",
            "133/133 [==============================] - 34s 239ms/step - loss: 0.9122\n",
            "Epoch 499/500\n",
            "Epoch 499/500\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.8937\n",
            "133/133 [==============================] - 33s 238ms/step - loss: 0.8937\n",
            "Epoch 500/500\n",
            "Epoch 500/500\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.8815\n",
            "133/133 [==============================] - 34s 240ms/step - loss: 0.8815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b92XbO0lIHCO"
      },
      "source": [
        "##P5. Generando texto nuevo usando la RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvIA_dH5IHs3"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl8wc_j7IJgh"
      },
      "source": [
        "#funcion para generar texto\n",
        "def generate_text(model, start_string):\n",
        "  #definimos cuantos tensores\n",
        "  num_generate=500\n",
        "  #convertimos el texto en números\n",
        "  input_eval=[char2idx[s] for s in start_string]\n",
        "  input_eval= tf.expand_dims (input_eval,0)\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 0.5  \n",
        "  #entre más alta la temperatura más creatividad al modelo, pero tambien\n",
        "  #más errores ortograficos.\n",
        "  model.reset_states()\n",
        "  #bucle para generar caracteres, mediante predicciones\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    input_eval= tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append (idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string+ ''.join(text_generated))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK4txFUZILxJ"
      },
      "source": [
        "###P5.1 generando texto "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdFlW3r_IOev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4175c1-6a40-4eee-b4f3-bfe18f6170ef"
      },
      "source": [
        "print(generate_text(model, start_string=u\"la interpretacion de los sueños\"))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "la interpretacion de los sueños         \n",
            "    sigmund freud \n",
            "el simbolismo prepara extrañar la relacion de este sueño. \n",
            "4) la representacion del material reciente, se halla en la mayor \n",
            "inversion a su montancia). \n",
            "la explicacion del sueño se agregan como una de las fantasias en absoluto de la inversion y se \n",
            "hallan mostradas a pesar de la general libidos de alguien que el material distinto ha laguna de al\n",
            "minicamente de por que el sueño se refiere, excitando a una persona con una señala». \n",
            "la elaboracion de los sueños pueden m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GznPp33DIQ-5"
      },
      "source": [
        "##P6.exportando modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsPxMdP9IS46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66aa7a2a-b86d-4f9b-a5a8-39413dae7aa4"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "import os\n",
        "# Serializamos el modelo en forma JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"modelRNN_cuentosV2.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/modelRNN_cuentosV2_pesos.hdf5\")\n",
        "model.save('modelRNN_cuentosV2.h5')\n",
        "print(\"modelo salvado en disco\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "modelo salvado en disco\n"
          ]
        }
      ]
    }
  ]
}